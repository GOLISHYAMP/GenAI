{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# installing the ollama3 of 1 billion paramater model\n",
    "<!-- !ollama run llama3.2:1b -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import AIMessagePromptTemplate\n",
    "from langchain_core.prompts.chat import HumanMessagePromptTemplate\n",
    "from langchain_core.prompts.chat import SystemMessagePromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A clever pattern to follow.\n",
      "\n",
      "The sequence appears to be \"a\" followed by \"b\", and then it repeats. So, if we continue:\n",
      "\n",
      "B (what comes after A) would be D.\n",
      "D (what comes after B) would be E.\n",
      "E (what comes after C) would be F.\n",
      "\n",
      "Therefore, the correct sequence is: A, B, D, E, F\n"
     ]
    }
   ],
   "source": [
    "SystemMessage = SystemMessagePromptTemplate.from_template(template='You are a intelligent bot')\n",
    "HumanMessage = HumanMessagePromptTemplate.from_template(template = \"what comes after A\")\n",
    "AIMessage = AIMessagePromptTemplate.from_template(template = \"It's B\")\n",
    "HumanMessage1 = HumanMessagePromptTemplate.from_template(template = \"what comes after C\")\n",
    "\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([SystemMessage, HumanMessage, AIMessage, HumanMessage1])\n",
    "\n",
    "chat = ChatOllama(\n",
    "    model = \"llama3.2:1b\",\n",
    "    temperature = 0.8,\n",
    "    num_predict = 200\n",
    ")\n",
    "# response = chat.invoke(template.format(salary = \"30000\"))\n",
    "response = chat.invoke(template.format())\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are ten potential song title ideas about parrots:\\n\\n1. \"Rainbow Wings\"\\n2. \"Paradise Found\"\\n3. \"Squawk Sanctuary\"\\n4. \"The Feathered Flock\"\\n5. \"Colorful Chaos\"\\n6. \"Whistle While You Work\" (a play on the parrot\\'s signature whistling)\\n7. \"In the Palm of Your Hand\"\\n8. \"Tropical Temptation\"\\n9. \"The Parrot\\'s Perch\"\\n10. \"Flock Together\"'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = OllamaLLM(model=\"llama3.2:1b\")\n",
    "model.invoke(\"Come up with 10 names for a song about parrots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers.list import CommaSeparatedListOutputParser\n",
    "output_parser = CommaSeparatedListOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.caches import InMemoryCache\n",
    "# Initialize cache\n",
    "cache = InMemoryCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here are five characteristics of lions:', 'Panthera leo', 'carnivorous mammal', 'social animal', 'nocturnal', 'apex predator']\n"
     ]
    }
   ],
   "source": [
    "SystemMessage = SystemMessagePromptTemplate.from_template(template=output_parser.get_format_instructions())\n",
    "HumanMessage = HumanMessagePromptTemplate.from_template(template = \"Just Give me five characteristics of {animal}.\")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([SystemMessage, HumanMessage])\n",
    "chat = ChatOllama(\n",
    "    model = \"llama3.2:1b\",\n",
    "    temperature = 0.8,\n",
    "    num_predict = 200,\n",
    "    cache = cache\n",
    ")\n",
    "print(output_parser.parse(chat.invoke(template.format(animal = \"lion\")).content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.datetime import DatetimeOutputParser\n",
    "dateTimeParser = DatetimeOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\\n\\nExamples: 1580-04-29T23:50:31.613388Z, 0666-01-05T17:37:56.325966Z, 1956-05-14T19:26:09.056534Z\\n\\nReturn ONLY this string, no other words!\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateTimeParser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-11 14:30:00\n"
     ]
    }
   ],
   "source": [
    "SystemMessage = SystemMessagePromptTemplate.from_template(template = \"Always provide the output in date only.\")\n",
    "HumanMessage = HumanMessagePromptTemplate.from_template(template = \"{request} {format}\")\n",
    "template = ChatPromptTemplate.from_messages([SystemMessage, HumanMessage])\n",
    "\n",
    "chat = ChatOllama(\n",
    "    model = \"llama3.2:1b\",\n",
    "    temperature = 0,\n",
    "    num_predict = 200,\n",
    "    cache = cache\n",
    ")\n",
    "\n",
    "print(dateTimeParser.parse(chat.invoke(template.format(request = \"TajMahal is built on which date?\", format = dateTimeParser.get_format_instructions())).content))\n",
    "# misformatted = dateTimeParser.parse(chat.invoke(template.format(request = \"TajMahal is built on which date?\", format = dateTimeParser.get_format_instructions())).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.output_parsers.fix import OutputFixingParser\n",
    "# parser = OutputFixingParser.from_llm(llm = chat, parser= dateTimeParser)\n",
    "# parser.parse(misformatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in c:\\users\\spurusho\\downloads\\genai\\personal\\langchaintutorials\\venv\\lib\\site-packages (2.11.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\spurusho\\downloads\\genai\\personal\\langchaintutorials\\venv\\lib\\site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in c:\\users\\spurusho\\downloads\\genai\\personal\\langchaintutorials\\venv\\lib\\site-packages (from pydantic) (2.33.0)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\spurusho\\downloads\\genai\\personal\\langchaintutorials\\venv\\lib\\site-packages (from pydantic) (4.13.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\spurusho\\downloads\\genai\\personal\\langchaintutorials\\venv\\lib\\site-packages (from pydantic) (0.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating custom Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers.pydantic import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scientist(BaseModel):\n",
    "    name : str = Field(description=\"Name of the Scientist\")\n",
    "    discoveries : list = Field(description=\"Python list of discoveries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"name\": {\"description\": \"Name of the Scientist\", \"title\": \"Name\", \"type\": \"string\"}, \"discoveries\": {\"description\": \"Python list of discoveries\", \"items\": {}, \"title\": \"Discoveries\", \"type\": \"array\"}}, \"required\": [\"name\", \"discoveries\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "parser = PydanticOutputParser(pydantic_object = Scientist)\n",
    "print(parser.get_format_instructions() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a JSON instance that conforms to the provided schema:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"properties\": {\n",
      "    \"name\": {\n",
      "      \"description\": \"Name of the Scientist\",\n",
      "      \"title\": \"Name\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"discoveries\": {\n",
      "      \"description\": \"Python list of discoveries\",\n",
      "      \"items\": {},\n",
      "      \"title\": \"Discoveries\",\n",
      "      \"type\": \"array\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\"name\", \"discoveries\"]\n",
      "}\n",
      "```\n",
      "\n",
      "This object has two properties: `name` and `discoveries`. The `name` property is a string, and the `discoveries` property is an array of strings. Both are required.\n"
     ]
    }
   ],
   "source": [
    "# SystemMessage = SystemMessagePromptTemplate.from_template(template = parser.get_format_instructions())\n",
    "HumanMessage = HumanMessagePromptTemplate.from_template(template = \"{request} {format}\")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([HumanMessage])\n",
    "\n",
    "chat = ChatOllama(\n",
    "    model = \"llama3.2:1b\",\n",
    "    temperature = 0,\n",
    "    num_predict = 200,\n",
    "    cache = cache\n",
    ")\n",
    "\n",
    "print(chat.invoke(template.format(request = \"Tell me about famous scientist\", format = parser.get_format_instructions())).content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serializing\n",
    "## Saving and Loading the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['anything'] input_types={} partial_variables={} template='Tell me a {anything}'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Tell me a story'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "prompt =  PromptTemplate(template = \"Tell me a {anything}\", input_variables = ['anything'])\n",
    "print(prompt)\n",
    "prompt.format(anything = \"story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.save('prompt.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['anything'] input_types={} partial_variables={} template='Tell me a {anything}'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts.loading import load_prompt\n",
    "loaded_promt = load_prompt(path='prompt.json')\n",
    "print(loaded_promt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
