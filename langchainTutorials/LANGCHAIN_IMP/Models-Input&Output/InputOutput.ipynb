{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spurusho\\Downloads\\GenAI\\Personal\\langchainTutorials\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "model_id = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100\n",
    ")\n",
    "#Only supports text-generation, text2text-generation, summarization and translation for now.\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time it looked like the city still had that beautiful old structure, which was still there, if in fact the old building could be rebuilt.\n",
      "\n",
      "While others wondered why Zwei was here, Yagyu was waiting for Zhuyu to arrive at the same location and to find Yagyu waiting for him.\n",
      "\n",
      "\"How long's it going to take me to get my things up? I still need to know when my sister will call me back? It can take forever if I\n"
     ]
    }
   ],
   "source": [
    "# Generate text using LangChain\n",
    "prompt = \"Once upon a time\"\n",
    "response = hf.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Generation(text='once upon a time, and in a way, this has been my greatest dream of all,\" he writes. \"The idea of having a home with a good public hospital, without the risk of being separated from other residents, has never come to me as a goal, but it is as though I had given them up to the health plan by letting them live with family. What I now have is this: I hope I can stay the best part of my life after my departure. But I can never get over one')], [Generation(text='Long long ago, there live a king and a prince who never even made an appearance; but no one will think that any one has ever even been seen; for they were never kings because of no fear of them: but these people who are princes now, and live as princes by a treaty made by our ancestors in the third year of the first century B.C., they have an incredible dignity in all this world and in each part of the world. No one may ever take him up to a certain place, nor allow him to')]]\n"
     ]
    }
   ],
   "source": [
    "# For multiple prompts\n",
    "results = hf.generate([\"once upon a time\", \"Long long ago, there live a king\"])\n",
    "print(results.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
