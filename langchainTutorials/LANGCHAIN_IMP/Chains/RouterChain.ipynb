{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e13fac27",
   "metadata": {},
   "source": [
    "### Input  --> Router  --> LLM decide chain ---> Chain ---> output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0d378ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "beginner_template = '''You are a physics teacher who is really\n",
    "focused on beginners and explaining complex topics in simple to understand terms. \n",
    "You assume no prior knowledge. Here is the question\\n{input}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0cee17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_template = '''You are a world expert physics professor who explains physics topics\n",
    "to advanced audience members. You can assume anyone you answer has a \n",
    "PhD level understanding of Physics. Here is the question\\n{input}'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee837ae2",
   "metadata": {},
   "source": [
    "## Route Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5570407",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        'name':'advanced physics',\n",
    "        'description': 'Answers advanced physics questions',\n",
    "        'prompt_template':expert_template\n",
    "    },\n",
    "    {\n",
    "        'name':'beginner physics',\n",
    "        'description': 'Answers basic beginner physics questions',\n",
    "        'prompt_template':beginner_template\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f938310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_ollama.chat_models import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0492c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOllama(model= \"llama3.2:1b\", temperature= 0.5)\n",
    "destination_chains = {}  # key = \"name\" value = Chain object\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "    chain = LLMChain(llm = chat, prompt= prompt)\n",
    "    destination_chains[name] = chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c96ab931",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(template= \"{input}\")\n",
    "default_chain = LLMChain(llm = chat, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b553f7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router.multi_prompt import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain\n",
    "from langchain.chains.router.llm_router import RouterOutputParser\n",
    "from langchain_core.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3d56da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "C:\\Users\\spurusho\\AppData\\Local\\Temp\\ipykernel_23152\\2711233452.py:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  router_template = '''\n"
     ]
    }
   ],
   "source": [
    "router_template = '''\n",
    "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "empty: Replies to empty questions\n",
    "advanced physics: Answers advanced physics questions\n",
    "beginner physics: Answers basic beginner physics questions\n",
    "\n",
    "<< INPUT >>\n",
    "{input}\n",
    "\n",
    "<< OUTPUT >>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b174cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db4a77c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "router_chain = LLMRouterChain.from_llm(chat, router_prompt)\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "256635da",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef4d7c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spurusho\\AppData\\Local\\Temp\\ipykernel_23152\\2065248458.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chain.run(\"How do magnets work?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "advanced physics: {'input': 'What are the magnetic properties of a refrigerator magnet?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A fascinating topic, my inquisitive students. As we delve into the realm of magnetic properties, let's explore the characteristics of a refrigerator magnet.\\n\\nRefrigerator magnets, also known as neodymium (NdFeB) magnets, are a type of permanent magnet that exhibits unique magnetic properties due to its composition and structure.\\n\\nThe key component in a refrigerator magnet is the neodymium iron boron (NdFeB) alloy. This alloy consists of:\\n\\n* Neodymium (Nd): A rare earth element with a high magnetic moment, responsible for the magnet's strength.\\n* Iron (Fe): An essential element that provides ferromagnetic properties, allowing the magnet to retain its magnetic field over time.\\n* Boron (B): A light element that contributes to the alloy's stability and resistance to corrosion.\\n\\nThe NdFeB alloy has a specific crystal structure, with a tetragonal lattice arrangement. This structure leads to:\\n\\n1. **High magnetic permeability**: The NdFeB alloy exhibits high magnetic permeability, allowing it to concentrate magnetic fields.\\n2. **Low coercivity**: The magnet's coercive strength is relatively low, enabling it to retain its magnetic field even when exposed to external magnetic fields or temperature fluctuations.\\n3. **High remanence**: The NdFeB alloy retains a significant portion of the applied magnetic field after demagnetization, resulting in high magnetic remanence.\\n\\nWhen a refrigerator magnet is placed near a ferromagnetic material, such as iron or nickel, it induces a weak external magnetic field due to the transfer of magnetic energy. This phenomenon is known as electromagnetic induction.\\n\\nThe strength of the induced magnetic field depends on several factors, including:\\n\\n* The size and shape of the magnet\\n* The distance between the magnet and the ferromagnetic material\\n* The temperature of the surrounding environment\\n\\nIn general, refrigerator magnets are designed to operate within a specific temperature range (typically between 10°C and 50°C) to maintain their magnetic properties. At higher temperatures, the magnetic field strength decreases, leading to reduced performance.\\n\\nIn conclusion, the magnetic properties of a refrigerator magnet can be attributed to its neodymium iron boron alloy composition, which exhibits high magnetic permeability, low coercivity, and high remanence. These characteristics enable the magnet to concentrate magnetic fields and induce weak external magnetic fields in ferromagnetic materials, making it an effective tool for various applications, including magnetic levitation, data storage, and magnetic resonance imaging (MRI).\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"How do magnets work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05621a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Carrots are typically orange in color. They can vary in shade from a deep, rich orange to a more pale or yellowish orange, but the classic and most well-known color is a vibrant orange.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from operator import itemgetter\n",
    "# from typing import Literal\n",
    "# from typing_extensions import TypedDict\n",
    "\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "# from langchain_ollama.chat_models import ChatOllama\n",
    "# llm = ChatOllama(model = \"llama3.2:1b\", template = 0.5)\n",
    "\n",
    "# prompt_1 = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", \"You are an expert on animals.\"),\n",
    "#         (\"human\", \"{query}\"),\n",
    "#     ]\n",
    "# )\n",
    "# prompt_2 = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", \"You are an expert on vegetables.\"),\n",
    "#         (\"human\", \"{query}\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# chain_1 = prompt_1 | llm | StrOutputParser()\n",
    "# chain_2 = prompt_2 | llm | StrOutputParser()\n",
    "\n",
    "# route_system = \"Route the user's query to either the animal or vegetable expert.\"\n",
    "# route_prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", route_system),\n",
    "#         (\"human\", \"{query}\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "\n",
    "# class RouteQuery(TypedDict):\n",
    "#     \"\"\"Route query to destination.\"\"\"\n",
    "#     destination: Literal[\"animal\", \"vegetable\"]\n",
    "\n",
    "\n",
    "# route_chain = (\n",
    "#     route_prompt\n",
    "#     | llm.with_structured_output(RouteQuery)\n",
    "#     | itemgetter(\"destination\")\n",
    "# )\n",
    "\n",
    "# chain = {\n",
    "#     \"destination\": route_chain,  # \"animal\" or \"vegetable\"\n",
    "#     \"query\": lambda x: x[\"query\"],  # pass through input query\n",
    "# } | RunnableLambda(\n",
    "#     # if animal, chain_1. otherwise, chain_2.\n",
    "#     lambda x: chain_1 if x[\"destination\"] == \"animal\" else chain_2,\n",
    "# )\n",
    "\n",
    "# chain.invoke({\"query\": \"what color are carrots\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a09e8cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
